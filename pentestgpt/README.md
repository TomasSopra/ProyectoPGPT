
# PentestGPT

PentestGPT is a penetration testing tool powered by Large Language Models (LLMs), designed to automate and assist in the penetration testing process. This tool is ideal for low to mid-level pentesters, offering interactive guidance, explanations, and to-do lists to aid junior users.

## Getting Started

PentestGPT leverages OpenAI's models, specifically GPT-4, due to its superior performance and reasoning capabilities compared to other models. It is designed to solve easy to medium HackTheBox machines and other CTF challenges.

## General Information

### What is PentestGPT?

PentestGPT is a penetration testing tool empowered by Large Language Models (LLMs). It is designed to automate the penetration testing process and is built on top of the ChatGPT API. It operates in an interactive mode to guide penetration testers in both overall progress and specific operations.

### Why is it necessary to pay to use PentestGPT?

To achieve the best performance, it is recommended to use the GPT-4+ API, which requires a payment method linked to your OpenAI account.

### Why OpenAI (GPT-4+)?

After extensive evaluation, GPT-4 was found to outperform GPT-3.5 and other LLMs in terms of penetration testing reasoning. GPT-3.5 often fails at simple tasks, whereas GPT-4 provides more reliable results.

### Why not just use GPT-4 directly?

GPT-4 can suffer from loss of context as tests go deeper into the penetration process. PentestGPT addresses this issue by maintaining context throughout the testing process.

### Custom GPT models

It is possible to use custom GPT models. Check: `pentestgpt/utils/APIs/gpt4all_api.py`.

## Installation

### Installation via GIT with pip

1. Create a virtual environment if necessary:
   ```sh
   virtualenv -p python3 venv
   source venv/bin/activate
   ```
2. Install the project:
   ```sh
   pip3 install git+https://github.com/TomasBastanteFlor/PGPT
   ```
3. Ensure that you have linked a payment method to your OpenAI account. Export your API key:
   ```sh
   export OPENAI_API_KEY='<your key here>'
   ```
4. Test the connection:
   ```sh
   pentestgpt-connection
   ```
5. Start PentestGPT:
   ```sh
   pentestgpt
   ```

### Installation via Docker

From the application repository, use the provided Dockerfile to create the image:
```sh
docker build -t kali-pgpt .
```
Run the container in interactive mode with privileges:
```sh
docker run -it --rm --privileged kali-pgpt
```
Export the API key:
```sh
export OPENAI_API_KEY="<your key here>"
```

## Usage

Run PentestGPT using the latest model:
```sh
pentestgpt
```
To use GPT-3.5 API:
```sh
pentestgpt --reasoning_model=gpt-3.5-turbo-16k
```
To start, run:
```sh
pentestgpt --args
```

### Available Commands

- `--help`: Show the help message.
- `--reasoning_model`: Specify the reasoning model to use.
- `--parsing_model`: Specify the parsing model to use.
- `--useAPI`: Use OpenAI API (default: True).
- `--log_dir`: Set the customized log output directory (relative path).
- `--logging`: Enable/disable log sharing (default: False).

### Interactive Commands

- `terminal`: Run custom user commands.
- `next`: Execute the next step based on test results.
- `more`: Get more details on the current step.
- `todo`: Show the to-do list (WIP).
- `discuss`: Discuss with PentestGPT.
- `google`: Search on Google (WIP).
- `help`: Show the help message.
- `quit`: Exit the tool and save the output as a log file.

### Sub-task Handler Commands

- `help`: Show the help message.
- `brainstorm`: Brainstorm solutions for the local task.
- `discuss`: Discuss the local task.
- `google`: Search on Google (WIP).
- `continue`: Exit the subtask and continue the main testing session.

### Report and Logging

After completing the penetration testing, a report will be generated in the `logs` folder if you quit with the `quit` command. Session IDs for previous sessions are stored in the `logs/sessions` folder. To print the report in a human-readable format:
```sh
python3 utils/report_generator.py <log file>
```

## Custom Model Endpoints and Local LLMs

PentestGPT supports local LLMs, optimized for GPT-4.

- To use a local GPT4ALL model:
  ```sh
  pentestgpt --reasoning_model=gpt4all --parsing_model=gpt4all
  ```
- To select a particular model with GPT4ALL, update the `module_mapping` class in `pentestgpt/utils/APIs/module_import.py`.

Follow the examples of `module_import.py`, `gpt4all.py`, and `chatgpt_api.py` to create API support for your model.