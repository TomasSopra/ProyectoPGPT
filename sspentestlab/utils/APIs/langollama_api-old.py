# import dataclasses
# from typing import Any, Dict, List
# import loguru
# from langchain_community.llms import Ollama  # Importar Ollama desde Langchain
# from sspentestlab.utils.llm_api import LLMAPI

# logger = loguru.logger
# logger.remove()

# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id

# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 2  # Mantener 2 mensajes en el historial por limitaciones del modelo
#         self.conversation_dict: Dict[str, Conversation] = {}
#         # Configurar el modelo de Ollama con el nombre del modelo
#         self.model = Ollama(model="llama3.2:1b", base_url="http://localhost:11434")  # Usar el servidor local de Ollama

#     def _chat_completion_fallback(self, history: List) -> str:
#         """Método alternativo en caso de error, utilizando solo el último mensaje"""
#         try:
#             latest_message = history[-1]["content"]
#             # Usar generate directamente con el último mensaje
#             response = self.model.generate(prompt=latest_message)
#             return response["text"] if "text" in response else "No response"
#         except Exception as e:
#             logger.error(f"Fallback error: {e}")
#             return "Error generating response"

#     def _chat_completion(self, history: List) -> str:
#         """Manejo principal de la conversación"""
#         try:
#             latest_message = history[-1]["content"]
#             # Generar respuesta con Ollama
#             response = self.model.generate(prompt=latest_message)
#             return response["text"] if "text" in response else "No response"
#         except Exception as e:
#             logger.error(f"Chat completion error: {e}")
#             # En caso de error, usar fallback
#             return self._chat_completion_fallback(history)







import dataclasses
import os
import re
import time
from typing import Any, Dict, List, Tuple

import loguru
import openai
import tiktoken
from gpt4all import GPT4All
from langchain_community.llms import Ollama
from tenacity import *
#from langchain.schema.output_parser import StrOutputParser


from sspentestlab.config.chat_config import GPT4ALLConfig
from sspentestlab.utils.llm_api import LLMAPI

logger = loguru.logger
logger.remove()
# logger.add(level="WARNING", sink="logs/chatgpt.log")


@dataclasses.dataclass
class Message:
    ask_id: str = None
    ask: dict = None
    answer: dict = None
    answer_id: str = None
    request_start_timestamp: float = None
    request_end_timestamp: float = None
    time_escaped: float = None


@dataclasses.dataclass
class Conversation:
    conversation_id: str = None
    message_list: List[Message] = dataclasses.field(default_factory=list)

    def __hash__(self):
        return hash(self.conversation_id)

    def __eq__(self, other):
        if not isinstance(other, Conversation):
            return False
        return self.conversation_id == other.conversation_id

# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 2  # maintain 2 messages in the history due to gpt4all limitation.
#         self.conversation_dict: Dict[str, Conversation] = {}
#         self.model = Ollama(model="llama3.2:1b")
        
#         # Le pasa url al servidor, si no es asi, usa la predeterminada.
#         #self.model = Ollama(model="llama3.2:1b", base_url=ollama_server_url) if ollama_server_url else Ollama(model="llama3.2:1b")
        
        
class OLLAMAAPI(LLMAPI):
    def __init__(self, config_class, use_langfuse_logging=False):
        self.name = str(config_class.model)
        self.history_length = 2
        self.conversation_dict = {}
        self.model = Ollama(model="llama3.2:1b")

    def _chat_completion_fallback(self, history: List) -> str:
        """As a fallback, only complete the last message."""
        # response = self.model.generate(prompts=[history[-1]], top_k=self.history_length)
        # response = self.model.generate(prompts=history[-1]["content"], top_k=self.history_length)
        response = self.model.generate(prompts=[history[-1]["content"]], top_k=self.history_length)
        return response

    # def _chat_completion(self, history: List) -> str:
    #     try:
    #         with self.model.chat_session():
    #             latest_message = history[-1]["content"]
    #             print(f'lastest_message -> {lastest_message}')
    #             response = self.model.generate(prompts=[latest_message], top_k=self.history_length)
    #             print(f'response -> {response}')
    #             # response = self.model.generate(
    #             # prompt=latest_message, top_k=self.history_length
    #             # )
    #             # response = self.model.generate(
    #             #     prompts=[latest_message], top_k=self.history_length
    #             # )
    #             return response
    #     except Exception as e:
    #         logger.error(e)
    #         return self._chat_completion_fallback(history)
        
    
    def _chat_completion(self, history: List) -> str:
        try:
            with self.model.chat_session():
                latest_message = history[-1]["content"]
                
                # Generar la respuesta del modelo
                response = self.model.generate(prompts=[latest_message], top_k=self.history_length)
                
                # Usa StrOutputParser para extraer el texto
                str_parser = StrOutputParser()
                parsed_output = str_parser.parse(response)
                
                # Retorna el texto extraído
                return parsed_output
        except Exception as e:
            logger.error(e)
            return self._chat_completion_fallback(history)
        
        
        
def llm_result_to_text(llm_result) -> str:
    """Extracts the text from an LLMResult object."""
    if hasattr(llm_result, 'generations') and llm_result.generations:
        return llm_result.generations[0][0].text  # Obtenemos el texto generado
    else:
        return ""  # Si no hay texto, devuelve una cadena vacía

        
        
        
        
        
        
        
        
        
        
        
# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = (2) # maintain 2 messages in the history due to gpt4all limitation.
#         self.conversation_dict: Dict[str, Conversation] = {}
#         #self.model = GPT4All(config_class.model)
#         self.model = Ollama(model="llama3.1:latest")



#     def _chat_completion_fallback(self, history: List) -> str:
#         """As a fallback, only complete the last message."""
        
#         response = self.model.generate(prompt=history[-1], top_k=self.history_length)
#         return response



#     def _chat_completion(self, history: List) -> str:
#         try:
#             with self.model.chat_session():
#                 latest_message = history[-1]["content"]
#                 response = self.model.generate(
#                     prompt=latest_message, top_k=self.history_length
#                 )
#                 return response
#         except Exception as e:
#             logger.error(e)
#             return self._chat_completion_fallback(history)



# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 2  # maintain 2 messages in the history due to gpt4all limitation.
#         self.conversation_dict: Dict[str, Conversation] = {}
#         self.model = Ollama(model="llama3.1:latest")

#     def _chat_completion_fallback(self, history: List) -> str:
#         """As a fallback, only complete the last message."""
#         response = self.model.generate(prompt=history[-1]["content"], top_k=self.history_length)
#         return response

#     def _chat_completion(self, history: List) -> str:
#         try:
#             with self.model.chat_session():
#                 latest_message = history[-1]["content"]
#                 response = self.model.generate(
#                     prompt=latest_message, top_k=self.history_length
#                 )
#                 return response
#         except Exception as e:
#             logger.error(e)
#             return self._chat_completion_fallback(history)
