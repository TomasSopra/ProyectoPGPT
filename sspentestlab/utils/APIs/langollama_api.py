


######################################### LA API ES COMPLETAMENTE FUNCIONAL -------------- ASI SOLA FUNCIONA JEJEJE


# import dataclasses
# import os
# import re
# import time
# from typing import Any, Dict, List, Tuple

# import loguru
# import openai
# import tiktoken
# #from langchain_community.llms import Ollama
# from langchain_ollama import OllamaLLM
# from tenacity import *

# from sspentestlab.config.chat_config import GPT4ALLConfig
# from sspentestlab.utils.llm_api import LLMAPI

# logger = loguru.logger
# logger.remove()
# logger.add(level="WARNING", sink="logs/chatgpt.log")


# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None


# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id


# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 2  # mantener 2 mensajes en el historial
#         self.conversation_dict = {}
#         self.model = OllamaLLM(model="llama3.2:1b")

#     def _chat_completion_fallback(self, history: List) -> str:
#         """Como fallback, solo completamos el último mensaje."""
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             logger.debug(f"Fallback prompt: {history[-1]['content']}")
#             response = self.model(prompt=history[-1]["content"], top_k=self.history_length)
#             logger.debug(f"Fallback response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in fallback: {e}")
#             return "An error occurred in fallback."

#     def _chat_completion(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             with self.model.chat_session():
#                 latest_message = history[-1]["content"]
#                 logger.debug(f"Prompt: {latest_message}")
#                 response = self.model(prompt=latest_message, top_k=self.history_length)
#                 logger.debug(f"Response: {response}")
#                 return response
#         except Exception as e:
#             logger.error(f"Error in chat completion: {e}")
#             return self._chat_completion_fallback(history)



########################################### EL RAG ESTA IMPLEMENTADO CON LIBRERIA LANGCHAIN (EN CONSTRUCCIÓN) -------------


# import dataclasses
# import os
# import re
# import time
# from typing import Any, Dict, List, Tuple

# import loguru
# from langchain_ollama import OllamaLLM
# from langchain_community.vectorstores import Chroma
# from langchain_community.embeddings import OpenAIEmbeddings
# from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader, UnstructuredWordDocumentLoader, TextLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from tenacity import *

# from sspentestlab.config.chat_config import GPT4ALLConfig
# from sspentestlab.utils.llm_api import LLMAPI

# logger = loguru.logger
# logger.remove()

# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id


# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 2  # mantener 2 mensajes en el historial
#         self.conversation_dict = {}
#         self.model = OllamaLLM(model="llama3.2:1b")
#         self.document_dir = r"C:\Users\ciber03\PentestFiles\rag"  # Directorio de documentos
#         self.vector_store = self._initialize_rag(self.document_dir)

#     def _initialize_rag(self, document_dir):
#         """Inicializa el sistema de recuperación basado en documentos para archivos .txt, .pdf, y .docx."""
#         logger.info(f"Inicializando RAG con documentos desde {document_dir}")

#         # Listar todos los archivos en el directorio
#         documents = []
#         for root, _, files in os.walk(document_dir):
#             for file in files:
#                 file_path = os.path.join(root, file)
#                 if file.endswith(".txt"):
#                     loader = TextLoader(file_path)
#                     documents.extend(loader.load())
#                 elif file.endswith(".pdf"):
#                     loader = PyMuPDFLoader(file_path)
#                     documents.extend(loader.load())
#                 elif file.endswith(".docx"):
#                     loader = UnstructuredWordDocumentLoader(file_path)
#                     documents.extend(loader.load())

#         if not documents:
#             logger.error(f"No se encontraron documentos en {document_dir}")
#             return None

#         # Dividir los documentos en fragmentos manejables
#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
#         docs = text_splitter.split_documents(documents)

#         # Generar IDs para los documentos
#         ids = [f"doc_{i}" for i in range(len(docs))] if docs else []

#         # Crear el vector store con los embeddings
#         embeddings = OpenAIEmbeddings()  # Cambia a otro embedding si es necesario
#         vector_store = Chroma.from_documents(docs, embeddings, ids=ids)
#         return vector_store

#     def _chat_completion_fallback(self, history: List) -> str:
#         """Como fallback, solo completamos el último mensaje."""
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             logger.debug(f"Fallback prompt: {history[-1]['content']}")
#             response = self.model(prompt=history[-1]["content"], top_k=self.history_length)
#             logger.debug(f"Fallback response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in fallback: {e}")
#             return "An error occurred in fallback."

#     def _chat_completion(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             with self.model.chat_session():
#                 latest_message = history[-1]["content"]
#                 logger.debug(f"Prompt: {latest_message}")
#                 response = self.model(prompt=latest_message, top_k=self.history_length)
#                 logger.debug(f"Response: {response}")
#                 return response
#         except Exception as e:
#             logger.error(f"Error in chat completion: {e}")
#             return self._chat_completion_fallback(history)




######################## LLAMA A LA FUNCIÓN DEL RAG

# import dataclasses
# import os
# import re
# import time
# from typing import Any, Dict, List, Tuple

# import dataclasses
# import loguru
# from langchain_ollama import OllamaLLM
# from tenacity import *

# from sspentestlab.config.chat_config import GPT4ALLConfig
# from sspentestlab.utils.llm_api import LLMAPI
# from sspentestlab.utils.APIs.rag_utils import initialize_rag  # Importar el nuevo archivo con la función RAG

# logger = loguru.logger
# logger.remove()

# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id


# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 2  # mantener 2 mensajes en el historial
#         self.conversation_dict = {}
#         self.model = OllamaLLM(model="llama3.2:1b")
#         self.document_dir = r"C:\Users\ciber03\Desktop\PruebasLLAMA\rag"  # Directorio de documentos
#         self.vector_store = self._initialize_rag()

#     def _initialize_rag(self):
#         """Llama a la función de RAG desde rag_utils.py"""
#         return initialize_rag(self.document_dir)

#     def _chat_completion_fallback(self, history: List) -> str:
#         """Como fallback, solo completamos el último mensaje."""
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             logger.debug(f"Fallback prompt: {history[-1]['content']}")
#             response = self.model(prompt=history[-1]["content"], top_k=self.history_length)
#             logger.debug(f"Fallback response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in fallback: {e}")
#             return "An error occurred in fallback."

#     def _chat_completion(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             with self.model.chat_session():
#                 latest_message = history[-1]["content"]
#                 logger.debug(f"Prompt: {latest_message}")
#                 response = self.model(prompt=latest_message, top_k=self.history_length)
#                 logger.debug(f"Response: {response}")
#                 return response
#         except Exception as e:
#             logger.error(f"Error in chat completion: {e}")
#             return self._chat_completion_fallback(history)




################################################# PRUEBA DE RAG CON LLAMAINDEX Y LLAMA3.2 1B


# import dataclasses
# import os
# import re
# import time
# from typing import Any, Dict, List, Tuple

# import loguru
# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage, Settings
# from langchain_ollama import OllamaLLM
# from tenacity import *

# from sspentestlab.config.chat_config import GPT4ALLConfig
# from sspentestlab.utils.llm_api import LLMAPI

# logger = loguru.logger
# logger.remove()

# # Crear una instancia del modelo Ollama
# Settings.llm = OllamaLLM(model="llama3.2:1b")

# # ============================ LlamaIndex RAG Setup ============================
# class LlamaIndexRAG:
#     def __init__(self, document_dir: str, persist_dir: str):
#         """Initializes LlamaIndex with documents and sets up storage persistence."""
#         self.document_dir = document_dir
#         self.persist_dir = persist_dir
#         self.index = self._initialize_index()

#     def _initialize_index(self):
#         """Checks if index exists, otherwise creates a new one."""
#         if not os.path.exists(self.persist_dir):
#             # Si no existe la carpeta de persistencia, creamos el índice desde los documentos.
#             logger.info(f"Loading documents from: {self.document_dir}")
#             documents = SimpleDirectoryReader(self.document_dir).load_data()
#             print(type(documents[0]))
#             print(documents[0])
#             index = VectorStoreIndex.from_documents(documents)
#             index.storage_context.persist(persist_dir=self.persist_dir)  # Persistir el índice
#         else:
#             # Cargar el índice persistente
#             logger.info(f"Loading existing index from storage: {self.persist_dir}")
#             storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
#             index = load_index_from_storage(storage_context)
#         return index

#     def query_index(self, query: str):
#         """Queries the LlamaIndex for relevant information."""
#         query_engine = self.index.as_query_engine()
#         response = query_engine.query(query)
#         return str(response)

# # ============================ Ollama API Integration ============================
# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id


# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class=None, use_langfuse_logging=False):
#         """Inicializa el modelo de Ollama y configura el sistema de RAG."""
        
#         # self.name = str(config_class.model) # CAMBIAR
#         self.name = "llama3.2:3b"
#         self.history_length = 5  # Mantener 5 mensajes en el historial
#         self.conversation_dict = {}
        
#         # Configurar el modelo de Ollama
#         self.model = OllamaLLM(model="llama3.2:3b", num_ctx=4096, num_predict=512)  
        
#         # Directorios de almacenamiento y persistencia
#         self.document_dir = "C:\\Users\\ciber03\\PentestFiles\\rag"
#         self.persist_dir = "./storage"
        
#         # Configurar el sistema de RAG con LlamaIndex
#         self.rag_system = LlamaIndexRAG(self.document_dir, self.persist_dir)
        
#         # Declarar retrieved_info como un atributo de instancia para acceso global
#         self.retrieved_info = "" 

#     def _chat_completion_fallback(self, history: list) -> str:
#         """Fallback para generar una respuesta si ocurre un error durante la completitud de chat."""
#         try:
#             if not history:
#                 raise ValueError("El historial está vacío, no se puede generar una respuesta.")

#             # Usar el último mensaje en el historial para la respuesta de fallback
#             latest_message = history[-1]["content"]
#             response = self.model(prompt=latest_message, top_k=self.history_length)
#             return response
#         except Exception as e:
#             logger.error(f"Error en fallback: {e}")
#             print(f"Error -> {e}")
#             return "Ocurrió un error en el proceso de fallback."

#     def _chat_completion(self, history: list) -> str:
#         """Genera una respuesta basada en el historial de mensajes y el RAG."""
#         try:
#             if not history:
#                 raise ValueError("El historial está vacío, no se puede generar una respuesta.")
            
#             # Último mensaje en el historial como el prompt de entrada
#             latest_message = history[-1]["content"]
#             print(f"Último mensaje (latest_message) enviado al RAG: {latest_message}")

#             # Consultar el índice para obtener información relevante del RAG
#             retrieved_info = self.rag_system.query_index(latest_message)
#             self.retrieved_info = retrieved_info  # Guardar la información recuperada como atributo

#             # Imprimir la información recuperada del RAG
#             print(f"Información recuperada del RAG (retrieved_info): {retrieved_info}")

#             # Crear el prompt combinado con la información recuperada
#             combined_prompt = f"{latest_message}\n\nInformación de documentos:\n{retrieved_info}"
#             logger.debug(f"Prompt combinado: {combined_prompt}")
            
#             # Generar la respuesta utilizando el modelo de Ollama
#             response = self.model(prompt=combined_prompt, top_k=self.history_length)
#             print(f"Respuesta del modelo Ollama: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error en chat completion: {e}")
#             return self._chat_completion_fallback(history)




# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 5  # Mantener 2 mensajes en el historial
#         self.conversation_dict = {}
#         self.model = OllamaLLM(model="llama3.2:3b", num_ctx=4096, num_predict=512)  # Cambia esto según el modelo de Ollama que estés usando

#         # Configurar el RAG con LlamaIndex y persistencia de almacenamiento
#         self.document_dir = "C:\\Users\\ciber03\\PentestFiles\\rag"
#         self.persist_dir = "./storage"
#         self.rag_system = LlamaIndexRAG(self.document_dir, self.persist_dir)

#     def _chat_completion_fallback(self, history: List) -> str:
#         """Fallback para generar una respuesta si ocurre un error."""
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             logger.debug(f"Fallback prompt: {history[-1]['content']}")
#             response = self.model(prompt=history[-1]["content"], top_k=self.history_length)
#             logger.debug(f"Fallback response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in fallback: {e}")
#             return "An error occurred in fallback."

#     def _chat_completion(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             # Obtener la última entrada del chat
#             latest_message = history[-1]["content"]
#             logger.debug(f"Prompt: {latest_message}")
#             print(f"Prompt que se le pasa al Rag: {latest_message}")  # Añadir un print aquí


#             # Consultar primero el índice para obtener información relevante (RAG)
#             retrieved_info = self.rag_system.query_index(latest_message)
#             print(f"Información recuperada del RAG: {retrieved_info}")  # Añadir un print aquí

#             # Combinar la información recuperada con el mensaje original para enviar al modelo
#             combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
#             print(f"Prompt combinado: {combined_prompt}")
            
#             # Generar la respuesta utilizando el modelo de Ollama
#             response = self.model.prompt(prompt=combined_prompt, top_k=self.history_length)
#             logger.debug(f"Response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in chat completion: {e}")
#             return self._chat_completion_fallback(history)


######################################################## Funciona 100% con el rag integrado:


# import dataclasses
# import os
# import re
# import time
# from typing import Any, Dict, List, Tuple

# import loguru
# from langchain_ollama import OllamaLLM
# from tenacity import *

# from sspentestlab.utils.APIs.module_import import LangOllamaConfigClass
# from sspentestlab.config.chat_config import GPT4ALLConfig
# from sspentestlab.utils.llm_api import LLMAPI


# logger = loguru.logger
# logger.remove()

# ####============================ LlamaIndex RAG Setup ============================
# class LlamaIndexRAG:
#     def __init__(self, document_dir: str, persist_dir: str):
#         """Initializes LlamaIndex with documents and sets up storage persistence."""
#         self.document_dir = document_dir
#         self.persist_dir = persist_dir
#         self.index = self._initialize_index()

#     def _initialize_index(self):
#         """Checks if index exists, otherwise creates a new one."""
#         if not os.path.exists(self.persist_dir):
#             ##Si no existe la carpeta de persistencia, creamos el índice desde los documentos.
#             logger.info(f"Loading documents from: {self.document_dir}")
#             documents = SimpleDirectoryReader(self.document_dir).load_data()
#             print(type(documents[0]))
#             print(documents[0])
#             index = VectorStoreIndex.from_documents(documents)
#             index.storage_context.persist(persist_dir=self.persist_dir)  # Persistir el índice
#         else:
#             ##Cargar el índice persistente
#             logger.info(f"Loading existing index from storage: {self.persist_dir}")
#             storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
#             index = load_index_from_storage(storage_context)
#         return index

#     def query_index(self, query: str):
#         """Queries the LlamaIndex for relevant information."""
#         query_engine = self.index.as_query_engine()
#         response = query_engine.query(query)
#         return str(response)

# ####============================ Ollama API Integration ============================
# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id

# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class:LangOllamaConfigClass, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 5  # Mantener 5 mensajes en el historial
#         self.conversation_dict = {}
#         self.model = OllamaLLM(model="llama3.2:3b", num_ctx=4096, num_predict=512)  # Cambia esto según el modelo de Ollama que estés usando

#         ##Configurar el RAG con LlamaIndex y persistencia de almacenamiento
#         self.document_dir = "C:\\Users\\ciber03\\PentestFiles\\rag"
#         self.persist_dir = "./storage"
#         self.rag_system = LlamaIndexRAG(self.document_dir, self.persist_dir)
        
#         self.retrieved_info = "" 

#     def _combine_rag_with_prompt(self, latest_message: str, retrieved_info: str, temperature: 0.5) -> str:
#         """Combina el mensaje original con la información recuperada del RAG."""
#         if retrieved_info:
#             ##Si hay información recuperada, darle prioridad en el prompt
#             combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
#         else:
#             ##Si no hay información recuperada, usar solo el mensaje original
#             combined_prompt = latest_message
#         return combined_prompt

#     def _chat_completion_fallback(self, history: List) -> str:
#         """Fallback para generar una respuesta si ocurre un error."""
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             logger.debug(f"Fallback prompt: {history[-1]['content']}")
#             response = self.model(prompt=history[-1]["content"], top_k=self.history_length)
#             logger.debug(f"Fallback response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in fallback: {e}")
#             return "An error occurred in fallback."

#     def _chat_completion(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             ##Obtener la última entrada del chat
#             latest_message = history[-1]["content"]
#             logger.debug(f"Prompt: {latest_message}")

#             ##Consultar primero el índice para obtener información relevante (RAG)
#             retrieved_info = self.rag_system.query_index(latest_message)
#             self.retrieved_info = retrieved_info  # Asignar el resultado a self.retrieved_info
#             print(f"Información recuperada del RAG: {retrieved_info}")

#             ##Combinar la información recuperada con el mensaje original para enviar al modelo
#             combined_prompt = self._combine_rag_with_prompt(latest_message, retrieved_info)
#             print(f"Prompt combinado: {combined_prompt}")
            
#             ##Generar la respuesta utilizando el modelo de Ollama
#             response = self.model.prompt(prompt=combined_prompt, top_k=self.history_length)
#             logger.debug(f"Response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in chat completion: {e}")
#             return self._chat_completion_fallback(history)















# ################################## OLLAMA API CON RAG PERO EN DIFERENTE SCRIPT



import dataclasses
import os
import re
import time
from typing import Any, Dict, List, Tuple

import loguru
from langchain_ollama import OllamaLLM
from tenacity import *

from sspentestlab.utils.APIs.module_import import LangOllamaConfigClass
from sspentestlab.config.chat_config import GPT4ALLConfig
from sspentestlab.utils.llm_api import LLMAPI
from sspentestlab.utils.APIs.rag_utils import LlamaIndexRAG


logger = loguru.logger
logger.remove()

####============================ Ollama API Integration ============================
@dataclasses.dataclass
class Message:
    ask_id: str = None
    ask: dict = None
    answer: dict = None
    answer_id: str = None
    request_start_timestamp: float = None
    request_end_timestamp: float = None
    time_escaped: float = None

@dataclasses.dataclass
class Conversation:
    conversation_id: str = None
    message_list: List[Message] = dataclasses.field(default_factory=list)

    def __hash__(self):
        return hash(self.conversation_id)

    def __eq__(self, other):
        if not isinstance(other, Conversation):
            return False
        return self.conversation_id == other.conversation_id

class OLLAMAAPI(LLMAPI):
    def __init__(self, config_class:LangOllamaConfigClass, use_langfuse_logging=False):
        self.name = str(config_class.model)
        self.history_length = 5  # Mantener 5 mensajes en el historial
        self.conversation_dict = {}
        self.model = OllamaLLM(model="qwen2.5:0.5b", num_ctx=4096, num_predict=512, base_url="http://ollama_server_container:11434")  # Cambia esto según el modelo de Ollama que estés usando

        ##Configurar el RAG con LlamaIndex y persistencia de almacenamiento
        self.document_dir = "/app/rag_files"  # Documentos originales para RAG
        self.persist_dir = "/app/db_vectorizada"  # Base de datos vectorizada
        self.rag_system = LlamaIndexRAG(self.document_dir, self.persist_dir)
        
        self.retrieved_info = "" 

    def _combine_rag_with_prompt(self, latest_message: str, retrieved_info: str, temperature: 0.5) -> str:
        """Combina el mensaje original con la información recuperada del RAG."""
        if retrieved_info:
            ##Si hay información recuperada, darle prioridad en el prompt
            combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
        else:
            ##Si no hay información recuperada, usar solo el mensaje original
            combined_prompt = latest_message
        return combined_prompt

    def _chat_completion_fallback(self, history: List) -> str:
        """Fallback para generar una respuesta si ocurre un error."""
        try:
            if not history:
                raise ValueError("History is empty, cannot generate a response.")

            logger.debug(f"Fallback prompt: {history[-1]['content']}")
            response = self.model(prompt=history[-1]["content"], top_k=self.history_length)
            logger.debug(f"Fallback response: {response}")
            return response
        except Exception as e:
            logger.error(f"Error in fallback: {e}")
            return "An error occurred in fallback."

    def _chat_completion(self, history: List) -> str:
        try:
            if not history:
                raise ValueError("History is empty, cannot generate a response.")

            ##Obtener la última entrada del chat
            latest_message = history[-1]["content"]
            logger.debug(f"Prompt: {latest_message}")

            ##Consultar primero el índice para obtener información relevante (RAG)
            retrieved_info = self.rag_system.query_index(latest_message)
            self.retrieved_info = retrieved_info  # Asignar el resultado a self.retrieved_info
            print(f"Información recuperada del RAG: {retrieved_info}")

            ##Combinar la información recuperada con el mensaje original para enviar al modelo
            combined_prompt = self._combine_rag_with_prompt(latest_message, retrieved_info)
            print(f"Prompt combinado: {combined_prompt}")
            
            ##Generar la respuesta utilizando el modelo de Ollama
            response = self.model.prompt(prompt=combined_prompt, top_k=self.history_length)
            logger.debug(f"Response: {response}")
            return response
        except Exception as e:
            logger.error(f"Error in chat completion: {e}")
            return self._chat_completion_fallback(history)
























# ################################# HUGGING FACE WITH API ¿?



# import dataclasses
# import os
# import re
# import time
# from typing import Any, Dict, List, Tuple

# import loguru
# import requests
# from tenacity import *

# from sspentestlab.utils.APIs.module_import import LangOllamaConfigClass
# from sspentestlab.config.chat_config import GPT4ALLConfig
# from sspentestlab.utils.llm_api import LLMAPI
# from sspentestlab.utils.APIs.rag_utils import LlamaIndexRAG

# logger = loguru.logger
# logger.remove()

# ####============================ Ollama API Integration ============================
# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id

# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class: LangOllamaConfigClass, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 5  # Mantener 5 mensajes en el historial
#         self.conversation_dict = {}
        
#         # Huggingface Inference API setup
#         self.api_url = "https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B"
#         self.headers = {"Authorization": "Bearer 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"}
        
#         ##Configurar el RAG con LlamaIndex y persistencia de almacenamiento
#         self.document_dir = "C:\\Users\\ciber03\\PentestFiles\\rag"
#         self.persist_dir = "./storage"
#         self.rag_system = LlamaIndexRAG(self.document_dir, self.persist_dir)
        
#         self.retrieved_info = "" 

#     def _combine_rag_with_prompt(self, latest_message: str, retrieved_info: str, temperature: 0.5) -> str:
#         """Combina el mensaje original con la información recuperada del RAG."""
#         if retrieved_info:
#             ##Si hay información recuperada, darle prioridad en el prompt
#             combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
#         else:
#             ##Si no hay información recuperada, usar solo el mensaje original
#             combined_prompt = latest_message
#         return combined_prompt

#     def _query_huggingface_api(self, payload: dict) -> dict:
#         response = requests.post(self.api_url, headers=self.headers, json=payload)
#         return response.json()

#     def _chat_completion(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             ##Obtener la última entrada del chat
#             latest_message = history[-1]["content"]
#             logger.debug(f"Prompt: {latest_message}")

#             ##Consultar primero el índice para obtener información relevante (RAG)
#             retrieved_info = self.rag_system.query_index(latest_message)
#             self.retrieved_info = retrieved_info  # Asignar el resultado a self.retrieved_info
#             print(f"Información recuperada del RAG: {retrieved_info}")

#             ##Combinar la información recuperada con el mensaje original para enviar al modelo
#             combined_prompt = self._combine_rag_with_prompt(latest_message, retrieved_info)
#             print(f"Prompt combinado: {combined_prompt}")
            
#             ##Generar la respuesta utilizando la API de Huggingface
#             response = self._query_huggingface_api({"inputs": combined_prompt})
#             logger.debug(f"Response: {response}")
#             return response.get("generated_text", "No response generated.")
#         except Exception as e:
#             logger.error(f"Error in chat completion: {e}")
#             return self._chat_completion_fallback(history)

#     def _chat_completion_fallback(self, history: List) -> str:
#         """Fallback para generar una respuesta si ocurre un error."""
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             logger.debug(f"Fallback prompt: {history[-1]['content']}")
#             response = self._query_huggingface_api({"inputs": history[-1]["content"]})
#             logger.debug(f"Fallback response: {response}")
#             return response.get("generated_text", "An error occurred in fallback.")
#         except Exception as e:
#             logger.error(f"Error in fallback: {e}")
#             return "An error occurred in fallback."




























# ########################################## USO DE HUGGING FACE ¿EN LOCAL?

# import dataclasses
# import os
# import re
# import time
# from typing import Any, Dict, List, Tuple

# import loguru
# from langchain_ollama import OllamaLLM
# from tenacity import *

# from sspentestlab.utils.APIs.module_import import LangOllamaConfigClass
# from sspentestlab.config.chat_config import GPT4ALLConfig
# from sspentestlab.utils.llm_api import LLMAPI
# from sspentestlab.utils.APIs.rag_utils import LlamaIndexRAG
# from transformers import AutoModelForCausalLM, AutoTokenizer
# import torch

# access_token = "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"

# logger = loguru.logger
# logger.remove()

# ####============================ Ollama API Integration ============================
# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id

# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class:LangOllamaConfigClass, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 5  # Mantener 5 mensajes en el historial
#         self.conversation_dict = {}

#         # self.model = OllamaLLM(model="llama3.2:1b", num_ctx=4096, num_predict=512)  # Cambia esto según el modelo de Ollama que estés usando

#         # Load the Huggingface model and tokenizer
#         self.tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B")
#         self.model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-3B", token=access_token)

#         ##Configurar el RAG con LlamaIndex y persistencia de almacenamiento
#         self.document_dir = "C:\\Users\\ciber03\\PentestFiles\\rag"
#         self.persist_dir = "./storage"
#         self.rag_system = LlamaIndexRAG(self.document_dir, self.persist_dir)
        
#         self.retrieved_info = "" 

#     def _combine_rag_with_prompt(self, latest_message: str, retrieved_info: str, temperature: 0.5) -> str:
#         """Combina el mensaje original con la información recuperada del RAG."""
#         if retrieved_info:
#             ##Si hay información recuperada, darle prioridad en el prompt
#             combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
#         else:
#             ##Si no hay información recuperada, usar solo el mensaje original
#             combined_prompt = latest_message
#         return combined_prompt

#     def _chat_completion_fallback(self, history: List) -> str:
#         """Fallback para generar una respuesta si ocurre un error."""
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             logger.debug(f"Fallback prompt: {history[-1]['content']}")
#             inputs = self.tokenizer(history[-1]["content"], return_tensors="pt")
#             outputs = self.model.generate(**inputs, max_length=512)
#             response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
#             logger.debug(f"Fallback response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in fallback: {e}")
#             return "An error occurred in fallback."
            

#     def _chat_completion(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             ##Obtener la última entrada del chat
#             latest_message = history[-1]["content"]
#             logger.debug(f"Prompt: {latest_message}")

#             ##Consultar primero el índice para obtener información relevante (RAG)
#             retrieved_info = self.rag_system.query_index(latest_message)
#             self.retrieved_info = retrieved_info  # Asignar el resultado a self.retrieved_info
#             print(f"Información recuperada del RAG: {retrieved_info}")

#             ##Combinar la información recuperada con el mensaje original para enviar al modelo
#             combined_prompt = self._combine_rag_with_prompt(latest_message, retrieved_info)
#             print(f"Prompt combinado: {combined_prompt}")
            
#             ##Generar la respuesta utilizando el modelo de Huggingface
#             inputs = self.tokenizer(combined_prompt, return_tensors="pt")
#             outputs = self.model.generate(**inputs, max_length=512)
#             response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
#             logger.debug(f"Response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in chat completion: {e}")
#             return self._chat_completion_fallback(history)


# import os
# import dataclasses
# from typing import List
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from sspentestlab.utils.APIs.rag_utils import LlamaIndexRAG
# from sspentestlab.utils.llm_api import LLMAPI
# from sspentestlab.utils.APIs.module_import import LangOllamaConfigClass
# from transformers import AutoModel


# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id

# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class: LangOllamaConfigClass, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 5  # Mantener 5 mensajes en el historial
#         self.conversation_dict = {}
        
#         # Inicialización del modelo de Hugging Face
#         access_token = "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"
#         model_name = "meta-llama/Llama-3.2-3B"  # Cambia según el modelo que quieras usar
#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)
#         self.model = AutoModelForCausalLM.from_pretrained(model_name)
        
#         # Configuración para ejecutar en GPU si está disponible
#         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#         self.model.to(self.device)
        
#         # RAG inicialización
#         self.document_dir = "C:\\Users\\ciber03\\PentestFiles\\rag"
#         self.persist_dir = "./storage"
#         self.rag_system = LlamaIndexRAG(self.document_dir, self.persist_dir)
        
#         self.retrieved_info = ""

#     def generate_response(self, prompt: str, max_length: int = 512, temperature: float = 0.7):
#         """Genera una respuesta basada en un prompt usando el modelo de Hugging Face."""
#         inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(self.device)
#         outputs = self.model.generate(
#             inputs["input_ids"],
#             max_length=max_length,
#             temperature=temperature,
#             top_p=0.9,
#             pad_token_id=self.tokenizer.eos_token_id
#         )
#         response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
#         return response

#     def _chat_completion(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")
            
#             # Obtener el mensaje más reciente del historial
#             latest_message = history[-1]["content"]
#             print(f"Prompt: {latest_message}")
            
#             # Consultar el índice RAG para información relevante
#             retrieved_info = self.rag_system.query_index(latest_message)
#             self.retrieved_info = retrieved_info
#             print(f"Información recuperada del RAG: {retrieved_info}")
            
#             # Combinar información del RAG con el mensaje del usuario
#             combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
            
#             # Generar la respuesta usando el modelo de Hugging Face
#             response = self.generate_response(combined_prompt)
#             print(f"Respuesta generada: {response}")
#             return response
#         except Exception as e:
#             print(f"Error in chat completion: {e}")
#             return "An error occurred while generating the response."

#     def _combine_rag_with_prompt(self, latest_message: str, retrieved_info: str) -> str:
#         """Combina el mensaje original con la información recuperada del RAG."""
#         if retrieved_info:
#             combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
#         else:
#             combined_prompt = latest_message
#         return combined_prompt

#     def retrieve_and_process(self, query: str):
#         """Un método auxiliar para probar la consulta al RAG y generar respuesta."""
#         retrieved_info = self.rag_system.query_index(query)
#         combined_prompt = self._combine_rag_with_prompt(query, retrieved_info)
#         return self.generate_response(combined_prompt)









































# ######################################## PRUEBA EN BASE AL NOTEBOOK:

# import dataclasses
# import os
# from typing import List

# import loguru
# import os
# from sentence_transformers import SentenceTransformer
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.document_loaders import DirectoryLoader
# # from langchain_community.vectorstores import Chroma
# from langchain_chroma import Chroma
# # from langchain_community.embeddings import SentenceTransformerEmbeddings
# from langchain_huggingface import HuggingFaceEmbeddings
# # from langchain_ollama import ChatOllama
# from langchain_ollama import OllamaLLM
# from langchain.prompts import PromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.vectorstores import SKLearnVectorStore
# from sspentestlab.config.chat_config import GPT4ALLConfig
# from sspentestlab.utils.llm_api import LLMAPI

# logger = loguru.logger
# logger.remove()

# # ============================ Text Splitter and Vector Store Setup ============================

# class RAGApplication:
#     def __init__(self, document_dir: str):
#         """Inicializa RAG con documentos y persistencia de embeddings."""
#         self.document_dir = document_dir
#         self.retriever = self._initialize_retriever()

#     def _initialize_retriever(self):
#         """Inicializa el vector store retriever con embeddings de SentenceTransformer."""
#         # Inicializa el modelo de embeddings
#         embedder = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
        
#         # Carga y divide documentos
#         loader = DirectoryLoader(self.document_dir, glob="**/*.txt", show_progress=True)
#         documents = loader.load()
#         text_splitter = RecursiveCharacterTextSplitter(
#             chunk_size=200, chunk_overlap=20
#         )
#         doc_splits = text_splitter.split_documents(documents)

#         # Crea embeddings y almacénalos en un vector store persistente
#         # embeddings = [embedder.embed_query(doc.page_content) for doc in doc_splits]
#         # vectorstore = Chroma(
#         #     collection_name=doc_splits,
#         #     embedding_function=embeddings,
#         #     persist_directory="./storage"
#         # )
#         vectorstore = SKLearnVectorStore.from_documents(
#             documents=doc_splits,
#             embedding=embedder
#             # persist_path="./storage"
#         )
        
#         # vectorstore.persist()  # Guarda en el directorio de persistencia
#         return vectorstore.as_retriever(k=1)

#     def retrieve_documents(self, query: str) -> str:
#         """Recupera documentos relevantes para una consulta dada."""
#         documents = self.retriever.retrieve(query)
#         return "\n".join([doc.page_content for doc in documents])


# # ============================ Ollama LLM and RAG Setup ============================

# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id

# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, document_dir="C:\\Users\\ciber03\\PentestFiles\\rag", use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 5
#         self.conversation_dict = {}
#         self.model = OllamaLLM(model="llama3.2:3b", temperature=0)
#         self.rag_system = RAGApplication(document_dir)
#         self.use_langfuse_logging = use_langfuse_logging

#         # Define the RAG chain with prompt and model
#         prompt = PromptTemplate(
#             template="""You are an assistant for question-answering tasks.
#             Use the following documents to answer the question.
#             If you don't know the answer, just say that you don't know.
#             Use three sentences maximum and keep the answer concise:
#             Question: {question}
#             Documents: {documents}
#             Answer:
#             """,
#             input_variables=["question", "documents"]
#         )
        
#         # Print the initial prompt structure to observe it before execution
#         print("Prompt Template Structure:\n", prompt.template)
        
#         self.rag_chain = prompt | self.model | StrOutputParser()

#     def _combine_rag_with_prompt(self, latest_message: str, retrieved_info: str) -> str:
#         """Combina el mensaje original con la información recuperada."""
#         if retrieved_info:
#             combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
#         else:
#             combined_prompt = latest_message
#         return combined_prompt

#     def _chat_completion(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")
            
#             # Obtener y mostrar el último mensaje del usuario
#             latest_message = history[-1]["content"]
#             print("Latest Message:\n", latest_message)
            
#             # Recuperar información relevante y combinarla con el último mensaje
#             retrieved_info = self.rag_system.retrieve_documents(latest_message)
#             combined_prompt = self._combine_rag_with_prompt(latest_message, retrieved_info)
            
#             # Mostrar el prompt combinado antes de ejecutarlo
#             print("Combined Prompt:\n", combined_prompt)
            
#             # Invocar la cadena RAG con el prompt y los documentos
#             response = self.rag_chain.invoke({"question": latest_message, "documents": retrieved_info})
            
#             return response
#         except Exception as e:
#             logger.error(f"Error in chat completion: {e}")
#             return self._chat_completion_fallback(history)

#     def _chat_completion_fallback(self, history: List) -> str:
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")
#             response = self.model(prompt=history[-1]["content"], top_k=self.history_length)
#             return response
#         except Exception as e:
#             logger.error(f"Error in fallback: {e}")
#             return "An error occurred in fallback."



















# import dataclasses
# import os
# import re
# import time
# from typing import Any, Dict, List, Tuple

# import loguru
# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage
# from langchain_ollama import OllamaLLM
# from tenacity import *

# from sspentestlab.config.chat_config import GPT4ALLConfig
# from sspentestlab.utils.llm_api import LLMAPI

# logger = loguru.logger
# logger.remove()

# # Declaramos rag_info como variable global
# rag_info = ""  # Esta variable se llenará con la información del sistema RAG y será accesible globalmente

# # ============================ LlamaIndex RAG Setup ============================
# class LlamaIndexRAG:
#     def __init__(self, document_dir: str, persist_dir: str):
#         """Inicializa LlamaIndex con documentos y configura persistencia."""
#         self.document_dir = document_dir
#         self.persist_dir = persist_dir
#         self.index = self._initialize_index()

#     def _initialize_index(self):
#         """Verifica si existe un índice, si no lo crea."""
#         if not os.path.exists(self.persist_dir):
#             logger.info(f"Loading documents from: {self.document_dir}")
#             documents = SimpleDirectoryReader(self.document_dir).load_data()
#             index = VectorStoreIndex.from_documents(documents)
#             index.storage_context.persist(persist_dir=self.persist_dir)  # Persistir el índice
#         else:
#             # Cargar el índice persistente
#             logger.info(f"Loading existing index from storage: {self.persist_dir}")
#             storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
#             index = load_index_from_storage(storage_context)
#         return index

#     def query_index(self, query: str):
#         """Consulta LlamaIndex para obtener información relevante."""
#         query_engine = self.index.as_query_engine()
#         response = query_engine.query(query)
#         return str(response)

# # ============================ Ollama API Integration ============================
# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id

# class OLLAMAAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.history_length = 2
#         self.conversation_dict = {}
#         self.model = OllamaLLM(model="llama3.2:3b")  # Ajusta el modelo según tus necesidades

#         # Configurar el RAG con LlamaIndex y persistencia de almacenamiento
#         self.document_dir = "C:\\Users\\ciber03\\PentestFiles\\rag"
#         self.persist_dir = "./storage"
#         self.rag_system = LlamaIndexRAG(self.document_dir, self.persist_dir)

#     def _combine_rag_with_prompt(self, latest_message: str, retrieved_info: str) -> str:
#         """Combina el mensaje original con la información recuperada del RAG."""
#         if retrieved_info:
#             # Si hay información recuperada, darle prioridad en el prompt
#             combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
#         else:
#             # Si no hay información recuperada, usar solo el mensaje original
#             combined_prompt = latest_message
#         return combined_prompt

#     def _chat_completion_fallback(self, history: List) -> str:
#         """Fallback para generar una respuesta si ocurre un error."""
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             logger.debug(f"Fallback prompt: {history[-1]['content']}")
#             response = self.model.prompt(prompt=history[-1]["content"], top_k=self.history_length)
#             logger.debug(f"Fallback response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in fallback: {e}")
#             return "An error occurred in fallback."

#     def _chat_completion(self, history: List) -> str:
#         global rag_info  # Indicamos que estamos usando la variable global
#         try:
#             if not history:
#                 raise ValueError("History is empty, cannot generate a response.")

#             # Obtener la última entrada del chat
#             latest_message = history[-1]["content"]
#             logger.debug(f"Prompt: {latest_message}")

#             # Consultar primero el índice para obtener información relevante (RAG)
#             rag_info = self.rag_system.query_index(latest_message)  # Actualizamos la variable global rag_info
#             print(f"Información recuperada del RAG: {rag_info}")

#             # Combinar la información recuperada con el mensaje original para enviar al modelo
#             combined_prompt = self._combine_rag_with_prompt(latest_message, rag_info)
#             print(f"Prompt combinado: {combined_prompt}")
            
#             # Generar la respuesta utilizando el modelo de Ollama
#             response = self.model.prompt(prompt=combined_prompt, top_k=self.history_length)
#             logger.debug(f"Response: {response}")
#             return response
#         except Exception as e:
#             logger.error(f"Error in chat completion: {e}")
#             return self._chat_completion_fallback(history)

