import dataclasses
import os
from datetime import datetime
from typing import List
import torch
from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig
from module_import import Llama31ConfigClass

class LlamaLocalModel:
    """Declare Llama model class."""
    
    def __init__(self, config_class: Llama31ConfigClass):
        """Llama model initialization function."""
        
        print("INIT")
        self.name = config_class.model
        self.model_name_or_path = config_class.model_name_or_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print("Device:", self.device)

        # load model configuration from params.json.
        try:
            config_path = os.path.join(self.model_name_or_path, 'params.json')
            print(f"Loading config from {config_path}")
            config = LlamaConfig.from_json_file(config_path)
            print("Config loaded successfully.")
        except Exception as e:
            print(f"Failed to load config: {e}")
            return

        # initialize the model with the configuration.
        try:
            print("Initializing model...")
            self.model = LlamaForCausalLM(config=config).to(self.device)
            print("Model initialized.")
        except Exception as e:
            print(f"Failed to initialize model: {e}")
            return

        # load the tokenizer directly from the model path.
        try:
            print("Loading tokenizer...")
            self.tokenizer = LlamaTokenizer.from_pretrained(self.model_name_or_path)
            print("Tokenizer loaded.")
        except Exception as e:
            print(f"Failed to load tokenizer: {e}")
            return

        # load the model weights.
        try:
            weights_path = os.path.join(self.model_name_or_path, 'consolidated.00.pth')
            print(f"Loading weights from {weights_path}")
            state_dict = torch.load(weights_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            print("Weights loaded successfully.")
        except Exception as e:
            print(f"Failed to load weights: {e}")
            return

        self.model.eval()  # Set the model to evaluation mode
        print("Model set to evaluation mode.")



    def generate_text(self, prompt: str, max_tokens: int = 100) -> str:
        """Obtain response from the model."""
        
        print("Generating text...")
        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)

        try:
            outputs = self.model.generate(inputs['input_ids'], max_length=max_tokens)
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            print("Text generated successfully.")
            return response
        except Exception as e:
            print(f"An error occurred during text generation: {e}")
            return None



# necessary function to realize testing
if __name__ == "__main__":
    print("Starting test...")
    config = Llama31ConfigClass(log_dir="logs")
    print("Configuration loaded.")
    llama_model = LlamaLocalModel(config)
    print("Model initialized.")

    try:
        response = llama_model.generate_text("Generate a list of innovative marketing ideas.")
        if response:
            print("Response:", response.strip())
    except Exception as e:
        print(f"An error occurred: {e}")