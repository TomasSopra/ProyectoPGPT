import dataclasses
import os
import re
import time
from typing import Any, Dict, List, Tuple

import loguru
import openai
import tiktoken
#from langchain_community.llms import Ollama
from langchain_ollama import OllamaLLM
from tenacity import *

from sspentestlab.config.chat_config import GPT4ALLConfig
from sspentestlab.utils.llm_api import LLMAPI

logger = loguru.logger
logger.remove()
logger.add(level="WARNING", sink="logs/chatgpt.log")


@dataclasses.dataclass
class Message:
    ask_id: str = None
    ask: dict = None
    answer: dict = None
    answer_id: str = None
    request_start_timestamp: float = None
    request_end_timestamp: float = None
    time_escaped: float = None


@dataclasses.dataclass
class Conversation:
    conversation_id: str = None
    message_list: List[Message] = dataclasses.field(default_factory=list)

    def __hash__(self):
        return hash(self.conversation_id)

    def __eq__(self, other):
        if not isinstance(other, Conversation):
            return False
        return self.conversation_id == other.conversation_id


class OLLAMAAPI(LLMAPI):
    def __init__(self, config_class, use_langfuse_logging=False):
        self.name = str(config_class.model)
        self.history_length = 2  # mantener 2 mensajes en el historial
        self.conversation_dict = {}
        self.model = OllamaLLM(model="llama3.2:1b")

    def _chat_completion_fallback(self, history: List) -> str:
        """Como fallback, solo completamos el Ãºltimo mensaje."""
        try:
            if not history:
                raise ValueError("History is empty, cannot generate a response.")

            logger.debug(f"Fallback prompt: {history[-1]['content']}")
            response = self.model(prompt=history[-1]["content"], top_k=self.history_length)
            logger.debug(f"Fallback response: {response}")
            return response
        except Exception as e:
            logger.error(f"Error in fallback: {e}")
            return "An error occurred in fallback."

    def _chat_completion(self, history: List) -> str:
        try:
            if not history:
                raise ValueError("History is empty, cannot generate a response.")

            with self.model.chat_session():
                latest_message = history[-1]["content"]
                logger.debug(f"Prompt: {latest_message}")
                response = self.model(prompt=latest_message, top_k=self.history_length)
                logger.debug(f"Response: {response}")
                return response
        except Exception as e:
            logger.error(f"Error in chat completion: {e}")
            return self._chat_completion_fallback(history)