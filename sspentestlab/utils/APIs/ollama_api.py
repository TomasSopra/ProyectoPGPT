# import os
# import time
# import dataclasses
# import loguru
# from typing import List
# from ollama import Client
# from typing import Tuple


# # Inicializar el logger
# logger = loguru.logger
# logger.remove()

# @dataclasses.dataclass
# class Message:
#     ask_id: str = None
#     ask: dict = None
#     answer: dict = None
#     answer_id: str = None
#     request_start_timestamp: float = None
#     request_end_timestamp: float = None
#     time_escaped: float = None

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str = None
#     message_list: List[Message] = dataclasses.field(default_factory=list)

#     def __hash__(self):
#         return hash(self.conversation_id)

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id

# class OLLAMAAPI:
#     def __init__(self, config_class, **kwargs):
#         """Inicializa la API de Ollama conectándose al servidor local"""
#         self.name = str(config_class.model)
#         self.client = Client(host='http://localhost:11434')  # Cliente Ollama con conexión al host local
#         self.model = config_class.model
#         self.log_dir = config_class.log_dir
#         self.history_length = 5  # Mantener 5 mensajes en el historial
#         self.conversation_dict = {}
#         self.error_wait_time = config_class.error_wait_time
#         logger.add(sink=os.path.join(self.log_dir, "ollama.log"), level="WARNING")

#     def _chat_completion(self, history: List, temperature=0.5) -> str:
#         """Genera una respuesta utilizando Ollama y el modelo llama3.2:1b"""
#         messages = [{"role": msg["role"], "content": msg["content"]} for msg in history]

#         try:
#             # Enviar la solicitud al servidor de Ollama
#             response = self.client.chat(model=self.model, messages=messages)
#         except Exception as e:
#             logger.warning(f"Error durante la solicitud a Ollama: {e}. Esperando {self.error_wait_time} segundos.")
#             time.sleep(self.error_wait_time)
#             response = self.client.chat(model=self.model, messages=messages)

#         # Verificar si la respuesta es válida
#         if not response or "content" not in response:
#             logger.error("La respuesta de Ollama no es válida.")
#             raise Exception("Respuesta no válida del servidor Ollama.")

#         return response['content']

#     # def send_message(self, conversation_id: str, message: dict) -> str:
#     #     """Envía un mensaje a la conversación y devuelve la respuesta generada"""
#     #     if conversation_id not in self.conversation_dict:
#     #         self.conversation_dict[conversation_id] = Conversation(conversation_id=conversation_id)
        
#     #     conversation = self.conversation_dict[conversation_id]

#     #     # Añadir el nuevo mensaje al historial
#     #     conversation.message_list.append(Message(ask=message, ask_id=message['role']))
#     #     history = [msg.ask for msg in conversation.message_list[-self.history_length:]]

#     #     response_content = self._chat_completion(history)
        
#     #     # Añadir la respuesta al historial
#     #     conversation.message_list[-1].answer = {'role': 'assistant', 'content': response_content}

#     #     return response_content


#     def send_message(self, message: str, conversation_id: str = None) -> Tuple[str, str]:
#         """
#         Envía un mensaje al modelo Ollama a través de su API y devuelve el texto de respuesta y el ID de la conversación actualizada.

#         Args:
#             message (str): El mensaje que será enviado al modelo.
#             conversation_id (str, opcional): ID de la conversación actual. Si no se proporciona, se iniciará una nueva.

#         Returns:
#             Tuple[str, str]: El texto de respuesta y el ID de la conversación actualizada.
#         """
#         # Si no hay un ID de conversación, inicia una nueva conversación
#         if conversation_id is None:
#             conversation_id = self.start_conversation()

#         # Envía el mensaje al modelo a través del cliente Ollama
#         try:
#             response = self.client.chat(
#                 model=self.model_name,
#                 messages=[{"role": "user", "content": message}]
#             )
            
#             # Obtener el contenido del mensaje de respuesta
#             response_text = response["message"]["content"]
            
#             # Devuelve el texto de respuesta y el ID de la conversación
#             return response_text, conversation_id
        
#         except Exception as e:
#             print(f"Error al enviar el mensaje a Ollama: {e}")
#             raise Exception(f"Failed to send message to Ollama: {str(e)}")


from ollama import Client
from typing import Tuple  # Importar Tuple
import logging

logger = logging.getLogger(__name__)

class OLLAMAAPI:
    def __init__(self, config, use_langfuse_logging=False):
        self.model_name = config.model
        self.client = Client(host="http://localhost:11434")
        self.use_langfuse_logging = use_langfuse_logging
        logger.info(f"OLLAMAAPI initialized with model: {self.model_name}")
    
    def start_conversation(self) -> str:
        """
        Inicia una nueva conversación (esto puede ser personalizado según la implementación).
        """
        # Aquí deberías generar un ID de conversación si es necesario
        conversation_id = "new_conversation"
        return conversation_id

    def send_message(self, message: str, conversation_id: str = None) -> Tuple[str, str]:
        """
        Envía un mensaje al modelo Ollama a través de su API y devuelve el texto de respuesta y el ID de la conversación actualizada.
        
        Args:
            message (str): El mensaje que será enviado al modelo.
            conversation_id (str, opcional): ID de la conversación actual. Si no se proporciona, se iniciará una nueva.
        
        Returns:
            Tuple[str, str]: El texto de respuesta y el ID de la conversación actualizada.
        """
        # Si no hay un ID de conversación, inicia una nueva conversación
        if conversation_id is None:
            conversation_id = self.start_conversation()

        # Envía el mensaje al modelo a través del cliente Ollama
        try:
            response = self.client.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": message}]
            )
            
            # Obtener el contenido del mensaje de respuesta
            response_text = response["message"]["content"]
            
            # Devuelve el texto de respuesta y el ID de la conversación
            return response_text, conversation_id
        
        except Exception as e:
            logger.error(f"Error al enviar el mensaje a Ollama: {e}")
            raise Exception(f"Failed to send message to Ollama: {str(e)}")
