# import os
# import loguru
# from langchain_community.vectorstores import Chroma
# from langchain_community.embeddings import OpenAIEmbeddings
# from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredWordDocumentLoader, TextLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# logger = loguru.logger

# def initialize_rag(document_dir: str):
#     """Inicializa el sistema de recuperación basado en documentos para archivos .txt, .pdf, y .docx."""
#     logger.info(f"Inicializando RAG con documentos desde {document_dir}")

#     # Listar todos los archivos en el directorio
#     documents = []
#     for root, _, files in os.walk(document_dir):
#         for file in files:
#             file_path = os.path.join(root, file)
#             if file.endswith(".txt"):
#                 loader = TextLoader(file_path)
#                 documents.extend(loader.load())
#             elif file.endswith(".pdf"):
#                 loader = PyMuPDFLoader(file_path)
#                 documents.extend(loader.load())
#             elif file.endswith(".docx"):
#                 loader = UnstructuredWordDocumentLoader(file_path)
#                 documents.extend(loader.load())

#     if not documents:
#         logger.error(f"No se encontraron documentos en {document_dir}")
#         return None

#     # Dividir los documentos en fragmentos manejables
#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
#     docs = text_splitter.split_documents(documents)

#     # Generar IDs para los documentos
#     ids = [f"doc_{i}" for i in range(len(docs))] if docs else []

#     # Crear el vector store con los embeddings
#     embeddings = OpenAIEmbeddings()  # Cambia a otro embedding si es necesario
#     vector_store = Chroma.from_documents(docs, embeddings, ids=ids)
#     return vector_store

import dataclasses
import os
import re
import time
from typing import Any, Dict, List, Tuple
from rich.console import Console


import loguru
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage, Settings
from llama_index.core.node_parser import SentenceSplitter
from langchain_ollama import OllamaLLM
# from llama_index.embeddings.local import LocalEmbedding
from llama_index.embeddings.ollama import OllamaEmbedding
# from llama_index.embeddings.huggingface import HuggingFaceEmbedding



Settings.llm = OllamaLLM(model="llama3.2:1b")
# Settings.embed_model = OllamaLLM(model="all-minilm-l6-v2:latest")
# Settings.embed_model = LocalEmbedding(model_path="path/to/your/local/model")

ollama_embedding = OllamaEmbedding(
    model_name="tazarov/all-minilm-l6-v2-f32:latest", base_url="http://ollama_server_container:11434"
    # ollama_additional_kwargs={"mirostat": 0},
)

Settings.embed_model = ollama_embedding


# Settings.embed:model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Settings.chunk_size = 512
# Settings.chunk_overlap = 20 
Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)


logger = loguru.logger
logger.remove()

class LlamaIndexRAG:
    def __init__(self, document_dir: str, persist_dir: str):
        """Initializes LlamaIndex with documents and sets up storage persistence."""
        self.document_dir = document_dir
        self.persist_dir = persist_dir
        self.index = self._initialize_index()
        self.doc_mapping = {}  # Diccionario para mapear fragmentos con nombres de archivo
        self.console = Console()  # Inicialización de Rich Console


    def _initialize_index(self):
        """Checks if persist_dir exists and is not empty; otherwise, creates a new index."""
        if not os.path.exists(self.persist_dir) or not os.listdir(self.persist_dir):
            # Si no existe el directorio o está vacío, crear uno nuevo
            logger.info(f"Initializing new index with documents from: {self.document_dir}")
            documents = SimpleDirectoryReader(self.document_dir).load_data()
            
            # Crear mapeo de fragmentos a nombres de archivo
            for doc in documents:
                self.doc_mapping[doc.doc_id] = doc.extra_info.get("file_name", "Archivo desconocido")
            
            index = VectorStoreIndex.from_documents(documents)
            index.storage_context.persist(persist_dir=self.persist_dir)  # Persistir en disco
        else:
            # Cargar índice persistente
            logger.info(f"Loading existing index from storage: {self.persist_dir}")
            storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
            index = load_index_from_storage(storage_context)
        return index

    def query_index(self, query: str):
        """Queries the LlamaIndex for relevant information."""
        query_engine = self.index.as_query_engine(response_mode="tree_summarize")
        response = query_engine.query(query)

        # Extraer nombres de archivo asociados con los nodos relevantes
        relevant_files = set()
        for node_with_score in response.source_nodes:
            node = node_with_score.node

            # Intentar obtener el nombre del archivo desde metadata
            try:
                file_name = node.metadata.get('file_name', "Archivo desconocido")
                relevant_files.add(file_name)
            except AttributeError:
                print("El nodo no tiene metadatos asociados.")
                file_name = "Archivo desconocido"

            # Depurar información del nodo
            #print(f"\nNodo procesado: {node.__dict__}")
            print(f"Archivo asociado: {file_name}")

        # Mostrar los archivos relevantes
        if relevant_files:
            # print("\nResultados obtenidos de los siguientes archivos:")
            self.console.print(
                "\nResultados obtenidos de los siguientes archivos:",
                style="bold green"
            )
            for file in relevant_files:
                print(f"- {file}")
        else:
            print("\nNo se encontraron archivos relevantes.")

        return str(response)





# class LlamaIndexRAG:
#     def __init__(self, document_dir: str, persist_dir: str):
#         """Initializes LlamaIndex with documents and sets up storage persistence."""
#         self.document_dir = document_dir
#         self.persist_dir = persist_dir
#         self.index = self._initialize_index()

#     def _initialize_index(self):
#         """Checks if index exists, otherwise creates a new one."""
#         if not os.path.exists(self.persist_dir):
#             ##Si no existe la carpeta de persistencia, creamos el índice desde los documentos.
#             logger.info(f"Loading documents from: {self.document_dir}")
#             documents = SimpleDirectoryReader(self.document_dir).load_data()
#             print(type(documents[0]))
#             print(documents[0])
#             index = VectorStoreIndex.from_documents(documents)
#             index.storage_context.persist(persist_dir=self.persist_dir)  # Persistir el índice
#         else:
#             ##Cargar el índice persistente
#             logger.info(f"Loading existing index from storage: {self.persist_dir}")
#             storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
#             index = load_index_from_storage(storage_context)
#         return index

#     def query_index(self, query: str):
#         """Queries the LlamaIndex for relevant information."""
#         query_engine = self.index.as_query_engine()
#         response = query_engine.query(query)
#         return str(response)